{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Partitioning in Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2qzhF14sFTGJ",
        "QYlBIVh1FTI3",
        "sOBDu8O50LB1",
        "rYW-D0KL0TXP",
        "fecO_d1exWzc",
        "1tWqgYn3UQqr",
        "Q0Wt6IumUZ7Y",
        "k8Te6uMPVfyr",
        "QZKNFBSUXAVR",
        "puDrRoiSXAdS",
        "T3xLMYb4EIuZ",
        "PQDLf8ihFSwZ"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVbDWEU5s4PNSTGs01w/un",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Big-Data/blob/main/Data_Partitioning_in_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAgFqnZf2hB5"
      },
      "source": [
        "# âœ‚ï¸ Data partitioning in Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed computing** does not only means the âš™ï¸ **processing of data in multiple nodes in a distributed manner**, but also the **data is stored distributedly in different nodes**. Each node is responsible for processing the data it stores. **Data âœ‚ï¸ partitioning** plays a ğŸ— critical role in the ğŸš€ performance of the processing of huge volumes of data in Spark.\n",
        "\n",
        "To demonstrate Sparkâ€™s partitioning, weâ€™ll walk through an **exercise** application of a **simple log dataset**. \n",
        "\n",
        "The table of contents of this notebook is as follow:\n",
        "\n",
        "1. [â„¹ï¸ Introduction](#1)\n",
        "2. [ğŸ”€ Shuffle process](#2)\n",
        "3. [ğŸ“‹ Exercise](#3)\n",
        "    1. [ğŸ“¥ Running Pyspark in Colab](#3.1)\n",
        "    2. [ğŸ‘€ Description of the data](#3.2)\n",
        "    3. [ğŸ”¢ Number of partitions](#3.3)\n",
        "    4. [âœ‚ï¸ Repartition](#3.4)\n",
        "        1. [Repartition by number of partitions](#3.4.1)\n",
        "        2. [Coalesce](#3.4.2)\n",
        "        3. [Repartition (by column)](#3.4.3)\n",
        "            1. [`partitionBy`](#3.4.3.1)\n",
        "4. [ğŸ“„ Summary](#4)\n",
        "5. [ğŸ“• References](#5)"
      ],
      "metadata": {
        "id": "BOoGjT10bqmL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qzhF14sFTGJ"
      },
      "source": [
        "# â„¹ï¸ Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8gPPQUrFWlS"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In Spark, as we explained in the notebook [Introduction to Spark](https://nbviewer.org/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb), the data is distributed in the nodes or ğŸ‘·â€â™‚ï¸ workers. That is, **a dataset is distributed in several nodes through a technique called âœ‚ï¸ \"Partitioning\"**.\n",
        "\n",
        "- A **partition** of a dataset contains a **part of the dataset** and must be **stored in several nodes** (***replicas***) so that the availability of data is guaranteed in case of one node âŒ fails. \n",
        "\n",
        "- When a node ğŸ‘ fails or abandons the cluster, the partitions in that node are reassigned in a âš–ï¸ uniform manner across the remaining nodes. Similarly, when a node joins the cluster, the partitions must be rebalanced across the new machines.\n",
        "\n",
        "<center><img src='https://i.ibb.co/2k21PKN/1-R81ph-LYl-QIAkx-Ia-LRsz3-EQ.png'></center>\n",
        "\n",
        "In the image above, we have a dataset that is âœ‚ï¸ divided into 8 partitions. For each partition, there are 3 replicas, and each replica is stored in 3 different nodes. If, for example, the *Node-4* âŒ fails, the data is available in other nodes.\n",
        "\n",
        "- Each node must process the data that it stores and â¡ï¸ deliver the result to another node. The node that receives this result must âš™ï¸ process this data and deliver the new result to another node. \n",
        "\n",
        "- A node may contain different partitions.\n",
        "\n",
        "- When processing, Spark ğŸ–‡ assign a task to each partition, and each work subprocess only can process a task at a time.\n",
        "\n",
        "A further visual explanation of partitioning is available in this [video](https://youtu.be/4Gfl0WuONMY?list=PLjH60bdMRScnn16R3cbb2CLxkbf0a9OO5) by Jesse Anderson.\n",
        "\n",
        "There is a ğŸ‘ğŸ‘ **tradeoff when partitioning datasets**. Having too â¬† many partitions or too â¬‡ few is not an ideal solution. The number of partitions in spark should be decided ğŸ¤” thoughtfully based on the cluster ğŸ”© configuration and ğŸ“œ requirements of the application.\n",
        "\n",
        "|                        | ğŸ‘ cons |   \n",
        "|------------------------|---------|\n",
        "| â¬‡ **a few partitions** |It can cause the application does not use all available nodes in the cluster, causing work ğŸ¤¯ overload in some nodes    |   \n",
        "| â¬† **a lot of partitions** | It increases â¬† parallelism but it can provoke that each partition has â¬‡ less data or no data at all causing Spark manages too many small tasks and hence increasing the processing â³ time due to the heavy ğŸ”€ shuffle process | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYlBIVh1FTI3"
      },
      "source": [
        "# ğŸ”€ Shuffle process <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPrw_QrEPME5"
      },
      "source": [
        "Each time a ğŸšŒ [wide transformation](https://nbviewer.org/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb#3.3) is executed in the processing, a node must collect the result of several nodes to continue the aggregation process. The ğŸ”€ **shuffle process** occurs **between processing stages**. It tries to shuffle the results between nodes, but procuring that a single node has the necessary data to process â™»ï¸ efficiently.\n",
        "\n",
        "<center><img src='https://i.ibb.co/x22HWVY/1-EHYJ6-Ou5rv-QMz-Yl9-T0-JOv-A.png'></center>\n",
        "\n",
        "In the example of the image below, we are ğŸ”¢ counting the number of records by each color. In the first stage (*Map*) the data of each node is processed. Each node sorts the records according to its color and it â¡ï¸ delivers the result to the following node so that it continues the processing. Before delivering the results to the following node, the ğŸ”€ *Shuffle* process happens. It makes sure a node has the records of a single color so that the node can count them, and in this way, the *Reduce* process delivers the final result.\n",
        "\n",
        "- If data is partitioned in a lot of nodes, this implies a ğŸ’ª heavy Shuffle process. There would be a lot of data â¡ï¸ sending across nodes, increasing the processing â³ time. This can be avoided by making a proper âœ‚ï¸ repartition of the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aCbqFbmFTLh"
      },
      "source": [
        "# ğŸ“‹ Exercise <a name=\"3\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“¥ Running Pyspark inÂ Colab <a name=\"3.1\"></a> "
      ],
      "metadata": {
        "id": "sOBDu8O50LB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run spark in Colab, we need to first install all the dependencies, i.e. Apache Spark with Hadoop, Java 8, and Findspark to locate spark in the system."
      ],
      "metadata": {
        "id": "U7SB2ODXnSiw"
      }
    },
    {
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "!tar xvf \"spark-3.0.3-bin-hadoop3.2.tgz\"\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we installed Spark and Java, we set the environment path which enables us to run Pyspark."
      ]
    },
    {
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's run a local spark session to test our installation:"
      ]
    },
    {
      "metadata": {
        "id": "9_Uz1NL4gHFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a2f9df-be2a-45c9-cfc2-59b02c34b383"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.2.1 in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark==3.2.1) (0.10.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MBqulnaqWGco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init('/content/spark-3.0.3-bin-hadoop3.2')\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "z9nK1zrJqNUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca58fe9-4494-4450-e969-915b180529cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fc19ff73210>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tree"
      ],
      "metadata": {
        "id": "gWFYvecMDZ6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‘€ Description of the data <a name=\"3.2\"></a> "
      ],
      "metadata": {
        "id": "rYW-D0KL0TXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **dataset** we are going to use for the exercise contains **logs information** of an application. It's stored in a **single file** in parquet format. Let's ğŸ“¥ download the file, load it in a PySpark dataframe, and display some records. "
      ],
      "metadata": {
        "id": "34t6SDpwbOjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"data\"\n",
        "!mkdir {data_path}\n",
        "!wget -q https://github.com/victorviro/Big-Data/raw/main/files/logs2.snappy.parquet -P {data_path}/"
      ],
      "metadata": {
        "id": "2Zt1Cda50U8_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs = spark.read.parquet(data_path)\n",
        "print(f'Number of records: {logs.count()}')\n",
        "logs.show(10, False)"
      ],
      "metadata": {
        "id": "w7aAj58t0hnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925c69d3-3847-4fdd-e6ed-7b5451426732"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records: 2000\n",
            "+----------+------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|date      |time        |severity|message                                                                                                                                                                |\n",
            "+----------+------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|2019-01-25|18:01:47,978|INFO    |Created MRAppMaster for application appattempt_1445144423722_0020_000001                                                                                               |\n",
            "|2016-10-07|18:01:48,963|INFO    |Executing with tokens:                                                                                                                                                 |\n",
            "|2017-10-13|18:01:48,963|INFO    |Kind                                                                                                                                                                   |\n",
            "|2016-05-10|18:01:49,228|INFO    |Using mapred newApiCommitter.                                                                                                                                          |\n",
            "|2017-06-18|18:01:50,353|INFO    |OutputCommitter set in config null                                                                                                                                     |\n",
            "|2016-05-30|18:01:50,509|INFO    |OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter                                                                                          |\n",
            "|2016-02-12|18:01:50,556|INFO    |Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler                             |\n",
            "|2015-12-03|18:01:50,556|INFO    |Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher                |\n",
            "|2017-06-21|18:01:50,556|INFO    |Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher              |\n",
            "|2016-08-10|18:01:50,556|INFO    |Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher|\n",
            "+----------+------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs.printSchema()"
      ],
      "metadata": {
        "id": "U_Q8E_CSwGMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452c4fdc-796a-4023-dc5c-ece3d781e459"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- time: string (nullable = true)\n",
            " |-- severity: string (nullable = true)\n",
            " |-- message: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a dataframe with 2000 records and 4 columns: *date*, *time*, *severity*, and *message*. \n",
        "\n",
        "| Column name | Description                      |   \n",
        "|-------------|----------------------------------|\n",
        "| date        | The ğŸ“… date of the log             |            \n",
        "| time        | The âŒšï¸ time of the log            |    \n",
        "| severity    | The ğŸš level of severity of the log |   \n",
        "| message     | The ğŸ’¬ text content of the log     |           "
      ],
      "metadata": {
        "id": "rk9rkURdbPCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”¢ Number of partitions <a name=\"3.3\"></a> "
      ],
      "metadata": {
        "id": "fecO_d1exWzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the number of partitions of the dataframe with the `getNumPartitions` method."
      ],
      "metadata": {
        "id": "QnSBrqpoxXJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(logs.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N38QTdbzYds-",
        "outputId": "ebb57232-eebd-4df2-86bd-a47b9212819e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, we are reading a single small file from our local disk so the ğŸ”¢ number of partitions is 1. However, if we read huger datasets, the number of partitions will â¬† increase. For example, \n",
        "- If we read a huge dataset from HDFS,  Spark, by default, creates one partition for each block of the file.\n",
        "- If we create a dataframe instead of reading it, the number of partitions of that dataset is defined by the `sc.defaultParallelism` ğŸ”© configuration value. The default value for this configuration is set to the number of all cores on all nodes in the cluster. On local, it is set to the number of cores on our system.\n",
        "\n",
        "We can store this dataframe in HDFS, S3, local disk... Let's store the dataframe on local disk with the following command:"
      ],
      "metadata": {
        "id": "6DqcW5uMzATk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"output_data/\"\n",
        "logs.write.mode(\"overwrite\").parquet(output_path)"
      ],
      "metadata": {
        "id": "niHFBAEUYkR9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this way the dataframe will be stored in local disk. Let's see the path where the dataframe was stored:"
      ],
      "metadata": {
        "id": "J35uBH_aZ_u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzBaI88aZqMH",
        "outputId": "a98fbe80-48bb-448b-ce6e-4370a2601239"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-48587dcb-b07e-4b5e-95f3-91663f9f3b8f-c000.snappy.parquet  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe is stored in 1 partition. If we load that single partition, it will contain all records of the dataset:"
      ],
      "metadata": {
        "id": "x1rwfVaXbg8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfpart00000 = spark.read.parquet(output_path +\"/*part-00000*\")\n",
        "dfpart00000.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HbOdrleaHHC",
        "outputId": "035504a6-8aa8-418e-8561-a3931b712e96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWqgYn3UQqr"
      },
      "source": [
        "## âœ‚ï¸ Repartition <a name=\"3.4\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Wt6IumUZ7Y"
      },
      "source": [
        "### Repartition by number of partitions <a name=\"3.4.1\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to â¬† **increase the number of partitions** so that more nodes can âš™ process these partitions, we can use the function [**`repartition`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.repartition.html). It **performs a ğŸ”€ shuffle operation** to reassign the data to new partitions. Let's redistribute the dataframe into 10 partitions and store it. "
      ],
      "metadata": {
        "id": "bQilSaJFa0hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = logs.repartition(10)\n",
        "logs.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(logs.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8vQyZmKa3m5",
        "outputId": "d4111399-dd5f-4094-807e-aaf0217a2fcd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the path where the dataframe was stored:"
      ],
      "metadata": {
        "id": "n3qyKr1DbHZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUxMMYD3bJaT",
        "outputId": "d79dbfc3-214e-4436-e315-67fbe2815ad5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00001-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00002-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00003-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00004-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00005-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00006-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00007-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00008-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "part-00009-c40203b0-22a2-4c7c-a2e1-26dc4191a567-c000.snappy.parquet\n",
            "_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF6vsauRUes5"
      },
      "source": [
        "A new hash code was created for the dataframe. Note the **hash** in the name of the partitions **is the same**, meaning they are **partitions of the same dataframe**. Now each partition contains $\\frac{1}{10}$ part of the data (200 records)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfpart00001 = spark.read.parquet(f'{output_path}/*part-00001*')\n",
        "dfpart00001.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1PMMt-wbUUH",
        "outputId": "60d3e23c-d11a-4f59-f954-cb04b85a032c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will allow a single node to process less data, but it may â¬† increase the processing â³ time of the ğŸ”€ shuffle operation."
      ],
      "metadata": {
        "id": "8BgmvKKIbdP1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Te6uMPVfyr"
      },
      "source": [
        "### Coalesce <a name=\"3.4.2\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method [**`coalesce`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html) is used to â¬‡ **decrease the number of partitions**. It does **not** perform the **ğŸ”€ shuffle operation**, so that it can't shuffle the data to new nodes. It can send data to nodes that already have part of the dataframe stored.\n",
        "\n",
        "If we try to repartition to â¬† more nodes with this function, the current number of partitions will be kept:"
      ],
      "metadata": {
        "id": "dBv26S_7b1et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = logs.coalesce(20)\n",
        "print(logs.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pazFlUgBcEUR",
        "outputId": "aa553add-e062-45a9-e506-5981f94ec076"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens if we try to â¬‡ decrease the number of partitions with the `coalesce` method."
      ],
      "metadata": {
        "id": "bw-CwgwqcNSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = logs.coalesce(5)\n",
        "logs.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(logs.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIofhwpcRMJ",
        "outputId": "eafeccb4-70be-45bf-cbc8-953861078210"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEvkWO98cYBO",
        "outputId": "a4133906-3534-4be1-d90f-4485249d7aff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-551196b8-c094-4d29-8b05-5b2613bd0957-c000.snappy.parquet\n",
            "part-00001-551196b8-c094-4d29-8b05-5b2613bd0957-c000.snappy.parquet\n",
            "part-00002-551196b8-c094-4d29-8b05-5b2613bd0957-c000.snappy.parquet\n",
            "part-00003-551196b8-c094-4d29-8b05-5b2613bd0957-c000.snappy.parquet\n",
            "part-00004-551196b8-c094-4d29-8b05-5b2613bd0957-c000.snappy.parquet\n",
            "_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50MrYPFZXASz"
      },
      "source": [
        "Each partition will contain the $\\frac{1}{5}$ part of the records (400)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfpart00001 = spark.read.parquet(f'{output_path}/*part-00001*')\n",
        "dfpart00001.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFklb209cbbJ",
        "outputId": "8a532ae7-cfa7-4f5b-c4ae-f28304f1681f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "â¬‡ Decreasing the number of partitions, a node must process more data, but the ğŸ”€ shuffle process will take less time."
      ],
      "metadata": {
        "id": "zMFl_NypchFD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZKNFBSUXAVR"
      },
      "source": [
        "### Repartition (by column) <a name=\"3.4.3\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's possible to use the function [**`repartition`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.repartition.html) to sort the table. It's necessary to ğŸ‘‰ identify a column for which group the data in a way so that a node has the data necessary for processing and the ğŸ”€ shuffle operation is minimum.\n",
        "\n",
        "Suppose we want to group by \"severity\", it will be convenient to partition the data for each level of severity and thus â™»ï¸ optimize the âš™ processing more than having the data partitioned by simply a number of partitions defined.\n",
        "\n",
        "This also allows for making **queries more efficiently**. If we want to filter the data for a value of the field \"severity\" we would **not have to read the whole dataframe**, only a partition that contains the data for that ğŸš level of severity. This function uses the ğŸ”€ **shuffle operation** to distribute the data."
      ],
      "metadata": {
        "id": "O9JTrPA2hUGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = logs.repartition(\"severity\")\n",
        "logs.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(logs.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpPGaTOQhWKG",
        "outputId": "e74da345-867b-4674-cf3a-85b5c6a3cfb2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Spark creates a minimum of 200 partitions, but in our case, only 5 files contain the information for each ğŸš level of severity. If it would be 10 levels of severity, it whould be 10 files with information."
      ],
      "metadata": {
        "id": "iqCy2uyJhg_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l --block-size=M {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5FkPWzphmZe",
        "outputId": "7642d9bb-a742-4af2-b639-8ce57dbd63fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1M\n",
            "-rw-r--r-- 1 root root 1M May  2 10:48 part-00000-10c2114f-1909-41d4-b979-79b1c2684aa4-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 1M May  2 10:48 part-00004-10c2114f-1909-41d4-b979-79b1c2684aa4-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 1M May  2 10:48 part-00015-10c2114f-1909-41d4-b979-79b1c2684aa4-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 1M May  2 10:48 part-00037-10c2114f-1909-41d4-b979-79b1c2684aa4-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 1M May  2 10:48 part-00095-10c2114f-1909-41d4-b979-79b1c2684aa4-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 0M May  2 10:48 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfpart00004 = spark.read.parquet(f'{output_path}/*part-00004*')\n",
        "dfpart00004.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FYahxjOh65i",
        "outputId": "43369c24-c973-4e9d-805d-074744cd12c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------+--------------------+\n",
            "|      date|        time|severity|             message|\n",
            "+----------+------------+--------+--------------------+\n",
            "|2019-01-25|18:01:47,978|    INFO|Created MRAppMast...|\n",
            "|2016-10-07|18:01:48,963|    INFO|Executing with to...|\n",
            "|2017-10-13|18:01:48,963|    INFO|                Kind|\n",
            "|2016-05-10|18:01:49,228|    INFO|Using mapred newA...|\n",
            "|2017-06-18|18:01:50,353|    INFO|OutputCommitter s...|\n",
            "+----------+------------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfpart00037 = spark.read.parquet(f'{output_path}/*part-00037*')\n",
        "dfpart00037.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I30lZzfziFVT",
        "outputId": "9c5d85a8-df7b-4b16-c4df-7ef034511016"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------+--------------------+\n",
            "|      date|        time|severity|             message|\n",
            "+----------+------------+--------+--------------------+\n",
            "|2017-02-14|18:05:27,570|    WARN|Address change de...|\n",
            "|2016-12-19|18:05:27,570|    WARN|Failed to renew l...|\n",
            "|2016-04-13|18:05:28,570|    WARN|Address change de...|\n",
            "|2016-05-12|18:05:28,570|    WARN|Failed to renew l...|\n",
            "|2017-02-06|18:05:29,570|    WARN|Address change de...|\n",
            "+----------+------------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must **use this function carefully** as a lot of unnecessary partitions will be created.\n",
        "\n",
        "In general, the partition by column is used to partition by multiple columns. Let's create 3 new columns: *Year, Month y Day*."
      ],
      "metadata": {
        "id": "s4vxJmOtiON6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "logs = logs.withColumn(\n",
        "    \"Year\", F.year(\"date\")).withColumn(\n",
        "    \"Month\", F.month(\"date\")).withColumn(\n",
        "    \"Day\", F.dayofmonth(\"date\")\n",
        ")\n",
        "logs = logs.repartition(\"Year\", \"Month\", \"Day\", \"severity\")\n",
        "logs.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(logs.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DgbEe39iTgI",
        "outputId": "162625fc-c59a-4334-bb56-460409184656"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this way, 200 partitions are created anyway, but the data is distributed across all partitions. This can imply ğŸš€ performance ğŸ› issues cause we will have **a lot of partitions with little data**."
      ],
      "metadata": {
        "id": "8X47dQ4Bi8E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l --block-size=K {output_path}"
      ],
      "metadata": {
        "id": "K6cOHDkUjBo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adJ9UmNBXAan"
      },
      "source": [
        "To improve this, the method [**`partitionBy`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html) is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puDrRoiSXAdS"
      },
      "source": [
        "#### [**`partitionBy`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html) <a name=\"3.4.3.1\"></a> "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method [**`partitionBy`**](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html) âœ‚ï¸ partitions the data in ğŸ“ **folders according** to the â†•ï¸ hierarchy of the **columns**, and thus **optimizing the query processes**."
      ],
      "metadata": {
        "id": "_aPGNag-jXlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs.coalesce(1).write.partitionBy(\"severity\").mode(\"overwrite\").parquet(output_path)\n",
        "logs.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KV8ibVHjbIR",
        "outputId": "648ced4a-f101-498e-ea72-7e313998f4db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------+------------------------------------------------------------------------+----+-----+---+\n",
            "|date      |time        |severity|message                                                                 |Year|Month|Day|\n",
            "+----------+------------+--------+------------------------------------------------------------------------+----+-----+---+\n",
            "|2016-08-26|18:01:50,666|INFO    |Default file system [hdfs://msra-sa-41:9000]                            |2016|8    |26 |\n",
            "|2016-08-26|18:01:50,728|INFO    |Emitting job history data to the timeline server is not enabled         |2016|8    |26 |\n",
            "|2016-08-26|18:01:53,885|INFO    |task_1445144423722_0020_m_000009 Task Transitioned from NEW to SCHEDULED|2016|8    |26 |\n",
            "|2015-12-11|18:01:57,291|INFO    |Opening proxy                                                           |2015|12   |11 |\n",
            "|2016-02-09|18:03:03,795|INFO    |Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is        |2016|2    |9  |\n",
            "+----------+------------+--------+------------------------------------------------------------------------+----+-----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe will keep its structure, but if we see the file system: **a ğŸ“ subdirectory is aggregated for each level of** severity, **the field that was used when partitioning**."
      ],
      "metadata": {
        "id": "lWAsGy2olUhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TopE2LRGDULK",
        "outputId": "849f22bf-17fa-40f0-ccc9-7cab77a4bc9d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_data/\n",
            "â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â””â”€â”€ part-00000-f6bb18f9-aaeb-468b-9e29-5a29dc4f388d.c000.snappy.parquet\n",
            "â”œâ”€â”€ severity=FATAL\n",
            "â”‚Â Â  â””â”€â”€ part-00000-f6bb18f9-aaeb-468b-9e29-5a29dc4f388d.c000.snappy.parquet\n",
            "â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â””â”€â”€ part-00000-f6bb18f9-aaeb-468b-9e29-5a29dc4f388d.c000.snappy.parquet\n",
            "â”œâ”€â”€ severity=WARN\n",
            "â”‚Â Â  â””â”€â”€ part-00000-f6bb18f9-aaeb-468b-9e29-5a29dc4f388d.c000.snappy.parquet\n",
            "â””â”€â”€ _SUCCESS\n",
            "\n",
            "4 directories, 5 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ğŸš« avoid a â¬† huge number of partitions, we first have re-partitioned to a smaller number of partitions (1) and then partition by column again. So each ğŸ“ subdirectory has 1 partition.\n",
        "\n",
        "In practice, the most **common way to partition data by columns** is partitioning by ğŸ“… **dates**."
      ],
      "metadata": {
        "id": "fqMYTJP8lfWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs.coalesce(1).write.partitionBy(\n",
        "    \"Year\",\"Month\",\"severity\"\n",
        ").mode(\"overwrite\").parquet(output_path)\n",
        "logs.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnDCpIR9l2At",
        "outputId": "fcbddba4-db80-4821-9698-cf3a49d9afe5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------+------------------------------------------------------------------------+----+-----+---+\n",
            "|date      |time        |severity|message                                                                 |Year|Month|Day|\n",
            "+----------+------------+--------+------------------------------------------------------------------------+----+-----+---+\n",
            "|2016-08-26|18:01:50,666|INFO    |Default file system [hdfs://msra-sa-41:9000]                            |2016|8    |26 |\n",
            "|2016-08-26|18:01:50,728|INFO    |Emitting job history data to the timeline server is not enabled         |2016|8    |26 |\n",
            "|2016-08-26|18:01:53,885|INFO    |task_1445144423722_0020_m_000009 Task Transitioned from NEW to SCHEDULED|2016|8    |26 |\n",
            "|2015-12-11|18:01:57,291|INFO    |Opening proxy                                                           |2015|12   |11 |\n",
            "|2016-02-09|18:03:03,795|INFO    |Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is        |2016|2    |9  |\n",
            "+----------+------------+--------+------------------------------------------------------------------------+----+-----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVtTfRtUDEEt",
        "outputId": "ccf5d909-d530-4306-f4a6-98d2a1ca2dbb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_data/\n",
            "â”œâ”€â”€ _SUCCESS\n",
            "â”œâ”€â”€ Year=2015\n",
            "â”‚Â Â  â”œâ”€â”€ Month=10\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=11\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â””â”€â”€ Month=12\n",
            "â”‚Â Â      â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â””â”€â”€ severity=WARN\n",
            "â”‚Â Â          â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”œâ”€â”€ Year=2016\n",
            "â”‚Â Â  â”œâ”€â”€ Month=1\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=10\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=11\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=12\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=2\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=3\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=4\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=5\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=6\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=7\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=8\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â””â”€â”€ Month=9\n",
            "â”‚Â Â      â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â””â”€â”€ severity=WARN\n",
            "â”‚Â Â          â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”œâ”€â”€ Year=2017\n",
            "â”‚Â Â  â”œâ”€â”€ Month=1\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=10\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=11\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=12\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=2\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=3\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=4\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=5\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=6\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=FATAL\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=7\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=8\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â””â”€â”€ Month=9\n",
            "â”‚Â Â      â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â””â”€â”€ severity=WARN\n",
            "â”‚Â Â          â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”œâ”€â”€ Year=2018\n",
            "â”‚Â Â  â”œâ”€â”€ Month=1\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=10\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=11\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=12\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=2\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=3\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=4\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=5\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=6\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=7\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=8\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â””â”€â”€ Month=9\n",
            "â”‚Â Â      â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â”œâ”€â”€ severity=FATAL\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â””â”€â”€ severity=WARN\n",
            "â”‚Â Â          â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”œâ”€â”€ Year=2019\n",
            "â”‚Â Â  â”œâ”€â”€ Month=1\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=10\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=11\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=2\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=3\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=4\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=5\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=6\n",
            "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â”œâ”€â”€ Month=7\n",
            "â”‚Â Â  â”‚Â Â  â””â”€â”€ severity=INFO\n",
            "â”‚Â Â  â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â  â””â”€â”€ Month=8\n",
            "â”‚Â Â      â”œâ”€â”€ severity=ERROR\n",
            "â”‚Â Â      â”‚Â Â  â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â”‚Â Â      â””â”€â”€ severity=INFO\n",
            "â”‚Â Â          â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "â””â”€â”€ Year=2020\n",
            "    â”œâ”€â”€ Month=12\n",
            "    â”‚Â Â  â””â”€â”€ severity=INFO\n",
            "    â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "    â”œâ”€â”€ Month=2\n",
            "    â”‚Â Â  â””â”€â”€ severity=WARN\n",
            "    â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "    â”œâ”€â”€ Month=6\n",
            "    â”‚Â Â  â””â”€â”€ severity=INFO\n",
            "    â”‚Â Â      â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "    â””â”€â”€ Month=7\n",
            "        â””â”€â”€ severity=INFO\n",
            "            â””â”€â”€ part-00000-7259f3e4-a1e5-4b3b-bc0b-745ccff44818.c000.snappy.parquet\n",
            "\n",
            "198 directories, 140 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This distributes the data â™»ï¸ **efficiently when we query filtering by these fields**. It will **not be necessary to read the whole dataset ** to process it, it will be enough to filter the fields that are required, and this optimizes the queries and the âŒ› time of the ğŸ”€ shuffle operation.\n",
        "\n",
        "If, for example, we want only the data for the year 2019, of month 2 and severity WARN, we will write the query in the following manner:"
      ],
      "metadata": {
        "id": "Eoot_urMmInw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.parquet(output_path + \"/Year=2019/Month=2/severity=WARN/\")\n",
        "df2.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQqOf_OPmPWf",
        "outputId": "a9aa532b-a8db-4b87-85a1-96dee1f00afd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------------------------------------------------------------------------------------------------+---+\n",
            "|date      |time        |message                                                                                                |Day|\n",
            "+----------+------------+-------------------------------------------------------------------------------------------------------+---+\n",
            "|2019-02-28|18:05:50,680|Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 53 seconds.  Will retry shortly ...|28 |\n",
            "+----------+------------+-------------------------------------------------------------------------------------------------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even we can use the trick `*` to amplify the range of the query. Let's query the information for all year 2019 for the severity WARN:"
      ],
      "metadata": {
        "id": "koCBz6lKmaxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.read.parquet(output_path + \"/Year=2019/Month=*/severity=WARN/\")\n",
        "df2.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8qdI6ajmeQT",
        "outputId": "67f9fa10-ff5d-49de-e94b-f2b9e9a58233"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------------------------------------------------------------------------------------------------------+---+\n",
            "|date      |time        |message                                                                                                 |Day|\n",
            "+----------+------------+--------------------------------------------------------------------------------------------------------+---+\n",
            "|2019-03-12|18:07:11,267|Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 133 seconds.  Will retry shortly ...|12 |\n",
            "|2019-05-07|18:10:22,013|Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 324 seconds.  Will retry shortly ...|7  |\n",
            "|2019-06-20|18:07:40,425|Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 162 seconds.  Will retry shortly ...|20 |\n",
            "|2019-02-28|18:05:50,680|Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 53 seconds.  Will retry shortly ... |28 |\n",
            "|2019-04-12|18:07:45,425|Address change detected. Old                                                                            |12 |\n",
            "+----------+------------+--------------------------------------------------------------------------------------------------------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“„ Summary <a name=\"4\"></a>"
      ],
      "metadata": {
        "id": "T3xLMYb4EIuZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or_DyOWpxY_q"
      },
      "source": [
        "By using âœ‚ï¸ partitions, the parellized use of the Spark cluster is â¬† maximized, and the storing space is â¬‡ reduced to achieve better ğŸš€ performance.\n",
        "\n",
        "When ğŸ¤” designing a **strategy** of partitioning (write partitions to a file system), we must take care of the access routes. For example, **is it common the usage of our partition keys (fields) in the filters?** Answering these â“ questions helps to determine by which column we must partition.\n",
        "\n",
        "However, partition does not mean, the more the best. **Spark recommends 2-3 tasks per each kernell of CPU** in the cluster. For instance, if we have 1000 kernells of CPU in our cluster, the recommended number of partitions is 2000-3000. Sometimes it depends on the distribution and asymmetry of the raw data. It must be adjusted to find the proper partitioning strategy. A proper partitioning strategy optimizes the query processes, grouping, and joining of datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDLf8ihFSwZ"
      },
      "source": [
        "# ğŸ“• References <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Spark SQL Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
        "\n",
        "- [Spark partitioning Understanding, sparkbyexamples](https://sparkbyexamples.com/spark/spark-partitioning-understanding/)\n",
        "\n",
        "- [Understanding HDFS using Legos](https://youtu.be/4Gfl0WuONMY?list=PLjH60bdMRScnn16R3cbb2CLxkbf0a9OO5)"
      ],
      "metadata": {
        "id": "TnMHfCEeBxXS"
      }
    }
  ]
}