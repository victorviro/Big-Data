{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zpMKsu_u6-_Z",
        "iiIvYZDWnsY4",
        "kTAxo-Y7aW8d",
        "Pa4gZEaGedQV",
        "x5KYvoH1A0vN",
        "cxwXT7Niy-pi",
        "W7q4LlSpV3S1",
        "EQ53x2lk4Gmk",
        "p-s467J5M7lj",
        "pvkMRedZ9B0G",
        "vJfswOll97yY",
        "6fBo9xyK9_pX",
        "5i4YPBpGKlHu",
        "E_Timw1k9B5Z",
        "CH3crTci9B7v"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ’¥ Introduction to Apache Spark"
      ],
      "metadata": {
        "id": "zpMKsu_u6-_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we introduce [**Apache Spark**](https://spark.apache.org/docs/latest/index.html), an open-source, **distributed processing system** used for **big data** workloads. First, we will see the main characteristics and ğŸ‘ benefits this framework offers us. Later, we will explain some ğŸ— key concepts around it. Finally, we explore the ğŸ— architecture and components that compound it. \n",
        "\n",
        "The table of contents of this notebook is as follow:\n",
        "\n",
        "1. [â„¹ï¸ Introduction](#1)\n",
        "2. [ğŸ—ƒ RDD, DataFrame and Dataset](#2)\n",
        "3. [ğŸ¬âš™ï¸ Actions and transformations](#3)\n",
        "    1. [âš¡ Caching](#3.1)\n",
        "    2. [â†•ï¸ RDD lineage graph](#3.2)\n",
        "    3. [ğŸğŸšŒ Narrow and wide transformations](#3.3)\n",
        "    4. [âš™ï¸ Jobs, stages and tasks](#3.4)\n",
        "    5. [â†”ï¸ DAG (Directed Acyclic Graph)](#3.5)\n",
        "4. [ğŸ— Spark architecture](#4)\n",
        "    1. [ğŸš— The driver](#4.1)\n",
        "    2. [ğŸ‘· Executors](#4.2)\n",
        "    3. [ğŸ•¹ Cluster manager](#4.3)\n",
        "    4. [ğŸ”› Launching a program](#4.4)\n",
        "    5. [ğŸ”ƒ Workflow](#4.5)\n",
        "5. [ğŸ“• References](#5)"
      ],
      "metadata": {
        "id": "jEbnhXooBr-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiIvYZDWnsY4"
      },
      "source": [
        "# â„¹ï¸ Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd71IU4HntqV"
      },
      "source": [
        "[***Apache Spark***](https://spark.apache.org/docs/latest/index.html) is a unified **analytics âš™ï¸ engine for large-scale data processing**. It provides â¬† high-level APIs in Java, Scala, Python, and R, and a rich set of higher-level ğŸ›  tools including \n",
        "\n",
        "- **Spark SQL** for SQL and structured data processing\n",
        "- **MLlib** for machine learning\n",
        "- **GraphX** for graph processing\n",
        "- Spark **Streaming** for incremental computation and stream processing\n",
        "\n",
        "These high-level tools are built on ğŸ” top of **Spark Core**, which is the base engine for distributed data processing. \n",
        "\n",
        "<center><img src='https://i.ibb.co/Dr8d908/spark-ecosystem.png'></center>\n",
        "\n",
        "Spark works **distributing the processing over a cluster** of servers or nodes. It can perform computations in **large data sets**. Briefly, Spark âœ‚ï¸ splits a computation into smaller tasks and distributes them to the nodes of the cluster. These nodes execute the tasks and â†©ï¸ deliver the result to the main node. \n",
        "\n",
        "Let's summarize the **main characteristics** of Spark.\n",
        "\n",
        "ğŸš„ ***Speed***\n",
        "\n",
        "Spark can manage PetaBytes of data at once, distributed across thousands of servers. It does it through **in-memory processing**, which makes it capable of ğŸšš deliver analysis in **real-time** at high âœˆ speed. Spark extends the MapReduce model by supporting more types of computations (e.g. stream processing or machine learning).\n",
        "\n",
        "ğŸ˜€ ***Friendly APIs***\n",
        "\n",
        "Spark provides instructions at a high level of abstraction. It is **compatible** with Java, Python, Scala and R. Spark can create distributed datasets from different file storage systems like HDFS, Amazon S3, HBase, or Cassandra.\n",
        "\n",
        "ğŸ˜´ ***Lazy evaluation*** \n",
        "\n",
        "Spark delays its evaluation until it is necessary. This is a ğŸ— key factor that contributes to its speed. It creates a ğŸ“‹ list of tasks (transformations) and it performs nothing until we ask it for the final result. Spark **aggregate the transformations in a computational DAG** (Directed Acyclic Graph) and they are **executed only when the controller asks for some data**.\n",
        "\n",
        "ğŸ’ğŸ» ***Scalability***\n",
        "\n",
        "Apache Spark is highly scalable. When we need more processing ğŸ’ª power, we â• **aggregate more servers to the cluster**. Instead of buying a super ğŸ–¥ computer able to accommodate our dataset (scaling-up), we rely on multiple computers, âœ‚ splitting the job between them (scaling out) which is less expensive and faster. To achieve this, Spark can run over different *cluster managers* as we'll see later.\n",
        "\n",
        "â˜ï¸ **Unified stack**\n",
        "\n",
        "Spark contains **multiple** tightly integrated **components** which provides various ğŸ‘ benefits:\n",
        "\n",
        "- Higher-level components (e.g. SQL or machine learning) benefit from improvements in the lower layers (core engine).\n",
        "\n",
        "- The ğŸ’° cost of development, maintenance, and ğŸš€ deployment is â¬‡ï¸ reduced drastically since we **don't have to manage independent systems** for performing different processing models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTAxo-Y7aW8d"
      },
      "source": [
        "# ğŸ—ƒ RDD, DataFrame and Dataset <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGPxNzE-ZR27"
      },
      "source": [
        "**RDDs**\n",
        "\n",
        "The main programming abstraction of Spark is **resilient distributed dataset (RDD)**, which is a collection of elements **âœ‚ï¸ partitioned across the nodes** of the cluster that **can be operated in parallel**. RDDs can contain any type of Python, Java, or Scala objects. An RDD can be **persisted in-memory** on worker nodes, allowing it to be â™»ï¸ reused efficiently across parallel operations. RDDs stands for:\n",
        "\n",
        "- ğŸ’ª ***Resilient***: They can recover quickly from any ğŸ› issues as the same data partitions are replicated across multiple worker nodes. Thus, even if one executor node âŒ fails, another will still process the data.\n",
        "- ***Distributed***: Data is distributed among different nodes in a cluster\n",
        "- ***Dataset***: Collection of partitioned data with values\n",
        "\n",
        "Once an RDD is created, it becomes **immutable**, that is, its state cannot be modified after it is created, but it can be transformed.\n",
        "\n",
        "In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. Under the hood, Spark ğŸ¤– automatically distributes the data contained in RDDs across our cluster and parallelizes the operations we perform on them.\n",
        "\n",
        "We can create RDDs by loading an external dataset or by distributing a collection of objects in the driver program. The last way is usually used only for ğŸš§ development/testing since it requires having an entire dataset in memory in one ğŸ’» machine.\n",
        "\n",
        "**Dataset and DataFrame** APIs provide the ğŸ‘ **benefits of RDDs** with the ğŸ‘ **benefits of Spark SQLâ€™s** optimized execution engine. Like an RDD, they are immutable distributed collections of data. Unlike an RDD, data is structured. When we ğŸ‘¨â€ğŸ’» develop Spark applications, **we typically use DataFrames and Datasets**. RDD still works internally within these APIs and it's important for the efficiency of Spark, but it's now used primarily for â¬‡ low-level tasks.\n",
        "\n",
        "In a **DataFrame**, data is **organized into named columns**, like a table in a relational database. It allows developers to **impose a structure** onto a distributed collection of data, and it provides a **friendly language API to manipulate our distributed data** (ğŸ‘‰ select columns, filter, ğŸ”— join, aggregate, etc) that allows us to solve common data analysis problems efficiently. \n",
        "\n",
        "**Datasets** are an extension of DataFrame API which provides static type-safe (applications can be âš ï¸ checked for errors before they are run), and an object-oriented programming interface.\n",
        "\n",
        "<center><img src='https://i.ibb.co/HhwNJxY/spark-rdd-df-dataset.jpg'></center>\n",
        "\n",
        "Further reading:\n",
        "- [A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
        "\n",
        "- [Apache Spark: 3 Reasons Why You Should Not Use RDDs](https://dzone.com/articles/apache-spark-3-reasons-why-you-should-not-use-rdds)\n",
        "\n",
        "- [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n",
        "\n",
        "- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¬âš™ï¸ Actions and transformations <a name=\"3\"></a>"
      ],
      "metadata": {
        "id": "Pa4gZEaGedQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs offer 2ï¸âƒ£ **types of operations**:\n",
        "\n",
        "- ***âš™ï¸ Transformations*** are operations that **create a new RDD from an existing one**. For example, `map` is a transformation that passes each dataset element through a function and returns a new RDD representing the results. Other examples of transformations are:\n",
        " - Adding a column to a DataFrame\n",
        " - Performing an aggregation or filtering\n",
        " - Computing summary statistics on a dataset\n",
        "\n",
        "- ***ğŸ¬ Actions***, on the other hand, **compute a result based on an RDD**, and either â†©ï¸ **return it** to the driver program or save it to an external storage system (e.g., HDFS). For example, `reduce` is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program. Other examples of actions are:\n",
        " - Printing information on the screen (e.g. `show` method)\n",
        " - Writing data to a hard drive or cloud bucket (e.g. `write` method)\n",
        " - Count the number of elements in a dataset (`count` method)\n",
        "\n",
        "**Transformations** in Spark are ğŸ˜´ **lazily computed**. Spark just remember the transformations applied to some dataset (ğŸ“ instructions) and they are only computed when we use them in an action. For example, a dataset transformed through `map` that later uses a `reduce` will return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
        "\n",
        "Actions force the evaluation of the transformations required for the RDD they were called on, since they need to actually produce output.\n",
        "\n",
        "<center><img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/09/Picture1-5-768x266.png'></center>\n",
        "\n",
        "\n",
        "This way of dealing with computations has many ğŸ‘ **benefits** with large scale data:\n",
        "\n",
        "- Storing ğŸ“‹ instructions in memory takes **less space** than storing intermediate data results. If we are performing many operations on a dataset and are ğŸ”™ returning the result data each step, we'll blow our storage faster although we don't need the intermediate results.\n",
        "\n",
        "- By having the list of operations to be performed, Spark can ğŸ’¡ optimize the work between executors more efficiently."
      ],
      "metadata": {
        "id": "Kl9DrhkzegtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš¡ Caching <a name=\"3.1\"></a>"
      ],
      "metadata": {
        "id": "x5KYvoH1A0vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs are by default â†©ï¸ recomputed each time we run an ğŸ¬ action on them. If we want to â™»ï¸ reuse an RDD in multiple actions, we can **persist it in memory** (partitioned across the machines of the cluster). If we'll not reuse the RDD, there is ğŸ™… no reason to persist it since we would waste storage space when Spark could instead stream through the data once and just compute the result."
      ],
      "metadata": {
        "id": "qVsuLqRlA3_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â†•ï¸ RDD lineage graph <a name=\"3.2\"></a>"
      ],
      "metadata": {
        "id": "cxwXT7Niy-pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we derive new RDDs from each other using transformations, Spark keeps ğŸ—’ track of the set of ğŸ–‡ dependencies between different RDDs, called the **lineage graph**.\n",
        "\n",
        "<center><img src='https://i.ibb.co/7Wk1R6y/spark-lineage-graph-example.png'></center>\n",
        "\n",
        "An RDD lineage graph is hence a **graph of** what **transformations** need to be executed after an ğŸ¬ action has been called. It starts with the ğŸ” earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute."
      ],
      "metadata": {
        "id": "TYECHcOSzDmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸğŸšŒ Narrow and wide transformations <a name=\"3.3\"></a>"
      ],
      "metadata": {
        "id": "W7q4LlSpV3S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2ï¸âƒ£ types of transformations:\n",
        "\n",
        "- ğŸ **Narrow transformations** are transformations where all the elements that are required to compute the records in a single partition live in the single partition of parent RDD. For example, `map` and `filter` are narrow transformations.\n",
        "\n",
        "- ğŸšŒ **Wide transformations** are transformations where all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. For example, `groupbyKey` and `join` are wide transformations.\n",
        "\n",
        "\n",
        "<center><img src='https://i.ibb.co/qFQvqPV/narrow-and-wide-transformations-spark.png'></center>\n",
        "\n",
        "**Narrow** transformations are âœˆ **faster** than wide transformations cause they **do not require any data ğŸ”€ shuffling** over the cluster network or no data movement. It is always good to keep in mind transformations that might require data shuffling (and hence slow ğŸ¢ down the process), and â¬‡ reduce the usage of wide transformations if it's possible."
      ],
      "metadata": {
        "id": "Q0SyGQ_uWEsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Jobs, stages and tasks <a name=\"3.4\"></a>"
      ],
      "metadata": {
        "id": "EQ53x2lk4Gmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we invoke an ğŸ¬ action on an RDD, a **job** is created. A job is divided into single or multiple stages, and stages are further âœ‚ï¸ divided into individual tasks.\n",
        "- A **job is divided into stages** based on the ğŸ”€ shuffle boundary. That is when Spark encounters a transformation that requires a shuffle (ğŸšŒ wide transformation) it creates a new stage.\n",
        "\n",
        "- Each stage is divided into tasks based on the ğŸ”¢ number of partitions in the RDD. \n",
        "\n",
        "- The **tasks** within a stage will **run in parallel. Every task** in the stage executes the same ğŸ“‹ set of instructions **over a 1 single partition**. So tasks are the smallest units of work in Spark.\n",
        "\n",
        "<center><img src='https://i.ibb.co/Bzbwhk7/spark-job.png'></center>\n",
        "\n",
        "\n",
        "The figure below ilustrates an example that shows how Spark computes job stages. Boxes with solid outlines are RDDs. Partitions are shaded rectangles, in black if they are cached.\n",
        "\n",
        "<center><img src='https://i.ibb.co/c39WQMk/stages-spark.png'></center>\n",
        "\n",
        "To run an action on RDD *G*, the scheduler builds stages at ğŸšŒ wide dependencies and **pipelines** ğŸ narrow transformations inside each stage. In this case, stage 1 does not need to run since B is cached, so we run 2 and then 3."
      ],
      "metadata": {
        "id": "5wi_W_z54J9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â†”ï¸ DAG (Directed Acyclic Graph) <a name=\"3.5\"></a>"
      ],
      "metadata": {
        "id": "p-s467J5M7lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **DAG** (Directed Acyclic Graph) is a directed graph with no ğŸ”„ directed cycles. In Spark, the âœ³ï¸ **vertices represent the RDDs**, and the â– **edges** represent the **operation** to be applied on RDD.\n",
        "\n",
        "During execution, Spark **transforms a â†•ï¸ logical execution plan ([RDD lineage](#3.1)) to a physical execution plan (â†”ï¸ DAG of stages)** performing several â™»ï¸ optimizations (such as \"pipelining\" transformations).\n",
        "\n",
        "\n",
        "<center><img src='https://i.ibb.co/0s0vy8M/dagscheduler-rdd-lineage-stage-dag.png'></center>"
      ],
      "metadata": {
        "id": "4-t1DFIvM9Ro"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvkMRedZ9B0G"
      },
      "source": [
        "# ğŸ— Spark architecture <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj9uUhsc_atT"
      },
      "source": [
        "In distributed mode, Spark follows a **master-slave architecture** with a cluster manager. A cluster has a master node and several ğŸ‘· worker nodes. In the master node is where the process called ğŸš— ***driver*** runs. The driver ğŸ—£ï¸ communicates with distributed workers which run processes called âš™ï¸ ***executors***.\n",
        "\n",
        "<center><img src='https://i.ibb.co/dkN1Tj6/spark-archi.png'></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸš— The driver <a name=\"4.1\"></a>"
      ],
      "metadata": {
        "id": "vJfswOll97yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ğŸš— **driver program** is the process that runs the ğŸ‘¨â€ğŸ’» code of our application that creates a Spark Context, RDDs, and perform transformations and ğŸ¬ actions.\n",
        "\n",
        "> The ***Spark Context*** is like a ğŸšª **gateway to the Spark functionalities**. It's similar to a database connection. Any command we execute in our database goes through the database connection. Likewise, anything we do on Spark goes through Spark context.\n",
        "\n",
        "The driver perform two duties:\n",
        "- **Convert** the user **program â¡ï¸ into tasks**. When the driver runs and find an action on an RDD, it will ğŸ”œ trigger a job to be run. \n",
        "\n",
        "  The driver then **converts the lineage graph into a set of stages** performing several optimizations (such as \"pipelining\" transformations). The tasks of the stages are blunded up and prepared to be â¡ï¸ sent to the cluster.\n",
        "\n",
        "- ğŸ•’ **Schedule tasks on ğŸ‘· executors**. The driver, which has a complete ğŸ‘€ view of the application's executors, coordinates the scheduling of individual tasks on executors. The drive will look at the current executors and try to schedule each task in an appropiate location, based on data placement.\n",
        "\n",
        "Moreover, the driver exposes information about the running Spark application through a **web interface**."
      ],
      "metadata": {
        "id": "sXoU_tk_k-85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‘· Executors <a name=\"4.2\"></a>"
      ],
      "metadata": {
        "id": "6fBo9xyK9_pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The executors are worker processes responsible for ğŸ”› **running the individual tasks** in a given Spark job. Executors have two roles:\n",
        "\n",
        "- They run the tasks and â†©ï¸ return results to the ğŸš— driver.\n",
        "\n",
        "- They provide **in-memory storage** for RDDs that are cached by ğŸ‘¨â€ğŸ’» user programs. Since RDDs are cached inside the executors, tasks can run alongside the cached data.\n",
        "\n",
        "A driver and its executors are ğŸ”— together termed a ***Spark application***."
      ],
      "metadata": {
        "id": "f3-ou_B0lGGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ•¹ Cluster manager <a name=\"4.3\"></a>"
      ],
      "metadata": {
        "id": "zWzvb9KG-Bsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A spark application is launched using an external service called cluster manager. The cluster manager launchs the ğŸ‘· executor processes. Spark can run ğŸ” on top of different cluster managers, such as YARN, Mesos, and Standalone cluster manager."
      ],
      "metadata": {
        "id": "hElavflllHHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”› Launching a program <a name=\"4.4\"></a>"
      ],
      "metadata": {
        "id": "5i4YPBpGKlHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark provides the script `spark-submit` to â¬†ï¸ **submit a program**. Through various options, `spark-submit` can ğŸ”— connect to different cluster managers and ğŸ•¹ control how many resources the application gets. For some cluster managers, `spark-submit` can run the ğŸš— driver within the cluster (e.g., on a YARN worker node), while for others, it can run it only on our ğŸ’» local machine."
      ],
      "metadata": {
        "id": "hYWEivUKKpF6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Timw1k9B5Z"
      },
      "source": [
        "## ğŸ”ƒ Workflow <a name=\"4.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ğŸš¶ walk through the steps that occur when we ğŸ”› run a Spark application on a cluster:\n",
        "\n",
        "1. The ğŸ‘¨â€ğŸ’» user submits an application using the `spark-submit` command.\n",
        "2. `spark-submit` launches the ğŸš— driver program and invokes the `main()` method specified by the user.\n",
        "3. The ğŸš— driver program ğŸ”Š contacts the ğŸ•¹ cluster manager to ask for resources to launch executors.\n",
        "4. The cluster manager launches ğŸ‘· executors on behalf of the driver program.\n",
        "5. The ğŸš— driver process runs through the user application. Based on the RDD ğŸ¬ actions and âš™ï¸ transformations in the program, the driver â¡ï¸ sends work to executors in the form of tasks.\n",
        "6. Tasks are run on ğŸ‘· executor processes to compute and save results or return them to the driver.\n",
        "7. If the driverâ€™s `main()` method exits or it calls `SparkContext.stop()`, it will ğŸ terminate the executors and release resources from the cluster manager."
      ],
      "metadata": {
        "id": "WUjH2InvJxOV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH3crTci9B7v"
      },
      "source": [
        "# ğŸ“• References <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3qKBAf9DHi"
      },
      "source": [
        "- [Spark documentation](https://spark.apache.org/docs/latest/cluster-overview.html)\n",
        "\n",
        "- [Book \"Learning Spark: Lightning-Fast Big Data Analysis\"](https://www.oreilly.com/library/view/learning-spark/9781449359034/)\n",
        "\n",
        "- [Online book \"apache-spark-internals\"](https://books.japila.pl/apache-spark-internals/)\n",
        "\n",
        "- [Databricks documentation](https://docs.databricks.com/getting-started/spark/index.html)\n",
        "\n",
        "- [Book \"Data Analysis with Python and PySpark\"](https://www.manning.com/books/data-analysis-with-python-and-pyspark)\n",
        "\n",
        "- [A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)](https://youtu.be/dmL0N3qfSc8)\n",
        "\n",
        "- [Advanced Apache Spark Training - Sameer Farooqui (Databricks)](https://youtu.be/7ooZ4S7Ay6Y)"
      ]
    }
  ]
}