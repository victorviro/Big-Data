{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_to_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zpMKsu_u6-_Z",
        "iiIvYZDWnsY4",
        "kTAxo-Y7aW8d",
        "Pa4gZEaGedQV",
        "x5KYvoH1A0vN",
        "cxwXT7Niy-pi",
        "W7q4LlSpV3S1",
        "EQ53x2lk4Gmk",
        "p-s467J5M7lj",
        "pvkMRedZ9B0G",
        "vJfswOll97yY",
        "6fBo9xyK9_pX",
        "5i4YPBpGKlHu",
        "E_Timw1k9B5Z",
        "CH3crTci9B7v"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💥 Introduction to Apache Spark"
      ],
      "metadata": {
        "id": "zpMKsu_u6-_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we introduce [**Apache Spark**](https://spark.apache.org/docs/latest/index.html), an open-source, **distributed processing system** used for **big data** workloads. First, we will see the main characteristics and 👍 benefits this framework offers us. Later, we will explain some 🗝 key concepts around it. Finally, we explore the 🏗 architecture and components that compound it. \n",
        "\n",
        "The table of contents of this notebook is as follow:\n",
        "\n",
        "1. [ℹ️ Introduction](#1)\n",
        "2. [🗃 RDD, DataFrame and Dataset](#2)\n",
        "3. [🎬⚙️ Actions and transformations](#3)\n",
        "    1. [⚡ Caching](#3.1)\n",
        "    2. [↕️ RDD lineage graph](#3.2)\n",
        "    3. [🏎🚌 Narrow and wide transformations](#3.3)\n",
        "    4. [⚙️ Jobs, stages and tasks](#3.4)\n",
        "    5. [↔️ DAG (Directed Acyclic Graph)](#3.5)\n",
        "4. [🏗 Spark architecture](#4)\n",
        "    1. [🚗 The driver](#4.1)\n",
        "    2. [👷 Executors](#4.2)\n",
        "    3. [🕹 Cluster manager](#4.3)\n",
        "    4. [🔛 Launching a program](#4.4)\n",
        "    5. [🔃 Workflow](#4.5)\n",
        "5. [📕 References](#5)"
      ],
      "metadata": {
        "id": "jEbnhXooBr-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiIvYZDWnsY4"
      },
      "source": [
        "# ℹ️ Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd71IU4HntqV"
      },
      "source": [
        "[***Apache Spark***](https://spark.apache.org/docs/latest/index.html) is a unified **analytics ⚙️ engine for large-scale data processing**. It provides ⬆ high-level APIs in Java, Scala, Python, and R, and a rich set of higher-level 🛠 tools including \n",
        "\n",
        "- **Spark SQL** for SQL and structured data processing\n",
        "- **MLlib** for machine learning\n",
        "- **GraphX** for graph processing\n",
        "- Spark **Streaming** for incremental computation and stream processing\n",
        "\n",
        "These high-level tools are built on 🔝 top of **Spark Core**, which is the base engine for distributed data processing. \n",
        "\n",
        "<center><img src='https://i.ibb.co/Dr8d908/spark-ecosystem.png'></center>\n",
        "\n",
        "Spark works **distributing the processing over a cluster** of servers or nodes. It can perform computations in **large data sets**. Briefly, Spark ✂️ splits a computation into smaller tasks and distributes them to the nodes of the cluster. These nodes execute the tasks and ↩️ deliver the result to the main node. \n",
        "\n",
        "Let's summarize the **main characteristics** of Spark.\n",
        "\n",
        "🚄 ***Speed***\n",
        "\n",
        "Spark can manage PetaBytes of data at once, distributed across thousands of servers. It does it through **in-memory processing**, which makes it capable of 🚚 deliver analysis in **real-time** at high ✈ speed. Spark extends the MapReduce model by supporting more types of computations (e.g. stream processing or machine learning).\n",
        "\n",
        "😀 ***Friendly APIs***\n",
        "\n",
        "Spark provides instructions at a high level of abstraction. It is **compatible** with Java, Python, Scala and R. Spark can create distributed datasets from different file storage systems like HDFS, Amazon S3, HBase, or Cassandra.\n",
        "\n",
        "😴 ***Lazy evaluation*** \n",
        "\n",
        "Spark delays its evaluation until it is necessary. This is a 🗝 key factor that contributes to its speed. It creates a 📋 list of tasks (transformations) and it performs nothing until we ask it for the final result. Spark **aggregate the transformations in a computational DAG** (Directed Acyclic Graph) and they are **executed only when the controller asks for some data**.\n",
        "\n",
        "💁🏻 ***Scalability***\n",
        "\n",
        "Apache Spark is highly scalable. When we need more processing 💪 power, we ➕ **aggregate more servers to the cluster**. Instead of buying a super 🖥 computer able to accommodate our dataset (scaling-up), we rely on multiple computers, ✂ splitting the job between them (scaling out) which is less expensive and faster. To achieve this, Spark can run over different *cluster managers* as we'll see later.\n",
        "\n",
        "☝️ **Unified stack**\n",
        "\n",
        "Spark contains **multiple** tightly integrated **components** which provides various 👍 benefits:\n",
        "\n",
        "- Higher-level components (e.g. SQL or machine learning) benefit from improvements in the lower layers (core engine).\n",
        "\n",
        "- The 💰 cost of development, maintenance, and 🚀 deployment is ⬇️ reduced drastically since we **don't have to manage independent systems** for performing different processing models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTAxo-Y7aW8d"
      },
      "source": [
        "# 🗃 RDD, DataFrame and Dataset <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGPxNzE-ZR27"
      },
      "source": [
        "**RDDs**\n",
        "\n",
        "The main programming abstraction of Spark is **resilient distributed dataset (RDD)**, which is a collection of elements **✂️ partitioned across the nodes** of the cluster that **can be operated in parallel**. RDDs can contain any type of Python, Java, or Scala objects. An RDD can be **persisted in-memory** on worker nodes, allowing it to be ♻️ reused efficiently across parallel operations. RDDs stands for:\n",
        "\n",
        "- 💪 ***Resilient***: They can recover quickly from any 🐛 issues as the same data partitions are replicated across multiple worker nodes. Thus, even if one executor node ❌ fails, another will still process the data.\n",
        "- ***Distributed***: Data is distributed among different nodes in a cluster\n",
        "- ***Dataset***: Collection of partitioned data with values\n",
        "\n",
        "Once an RDD is created, it becomes **immutable**, that is, its state cannot be modified after it is created, but it can be transformed.\n",
        "\n",
        "In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. Under the hood, Spark 🤖 automatically distributes the data contained in RDDs across our cluster and parallelizes the operations we perform on them.\n",
        "\n",
        "We can create RDDs by loading an external dataset or by distributing a collection of objects in the driver program. The last way is usually used only for 🚧 development/testing since it requires having an entire dataset in memory in one 💻 machine.\n",
        "\n",
        "**Dataset and DataFrame** APIs provide the 👍 **benefits of RDDs** with the 👍 **benefits of Spark SQL’s** optimized execution engine. Like an RDD, they are immutable distributed collections of data. Unlike an RDD, data is structured. When we 👨‍💻 develop Spark applications, **we typically use DataFrames and Datasets**. RDD still works internally within these APIs and it's important for the efficiency of Spark, but it's now used primarily for ⬇ low-level tasks.\n",
        "\n",
        "In a **DataFrame**, data is **organized into named columns**, like a table in a relational database. It allows developers to **impose a structure** onto a distributed collection of data, and it provides a **friendly language API to manipulate our distributed data** (👉 select columns, filter, 🔗 join, aggregate, etc) that allows us to solve common data analysis problems efficiently. \n",
        "\n",
        "**Datasets** are an extension of DataFrame API which provides static type-safe (applications can be ⚠️ checked for errors before they are run), and an object-oriented programming interface.\n",
        "\n",
        "<center><img src='https://i.ibb.co/HhwNJxY/spark-rdd-df-dataset.jpg'></center>\n",
        "\n",
        "Further reading:\n",
        "- [A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
        "\n",
        "- [Apache Spark: 3 Reasons Why You Should Not Use RDDs](https://dzone.com/articles/apache-spark-3-reasons-why-you-should-not-use-rdds)\n",
        "\n",
        "- [Apache Spark RDD vs DataFrame vs DataSet](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n",
        "\n",
        "- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎬⚙️ Actions and transformations <a name=\"3\"></a>"
      ],
      "metadata": {
        "id": "Pa4gZEaGedQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs offer 2️⃣ **types of operations**:\n",
        "\n",
        "- ***⚙️ Transformations*** are operations that **create a new RDD from an existing one**. For example, `map` is a transformation that passes each dataset element through a function and returns a new RDD representing the results. Other examples of transformations are:\n",
        " - Adding a column to a DataFrame\n",
        " - Performing an aggregation or filtering\n",
        " - Computing summary statistics on a dataset\n",
        "\n",
        "- ***🎬 Actions***, on the other hand, **compute a result based on an RDD**, and either ↩️ **return it** to the driver program or save it to an external storage system (e.g., HDFS). For example, `reduce` is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program. Other examples of actions are:\n",
        " - Printing information on the screen (e.g. `show` method)\n",
        " - Writing data to a hard drive or cloud bucket (e.g. `write` method)\n",
        " - Count the number of elements in a dataset (`count` method)\n",
        "\n",
        "**Transformations** in Spark are 😴 **lazily computed**. Spark just remember the transformations applied to some dataset (📝 instructions) and they are only computed when we use them in an action. For example, a dataset transformed through `map` that later uses a `reduce` will return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
        "\n",
        "Actions force the evaluation of the transformations required for the RDD they were called on, since they need to actually produce output.\n",
        "\n",
        "<center><img src='https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/09/Picture1-5-768x266.png'></center>\n",
        "\n",
        "\n",
        "This way of dealing with computations has many 👍 **benefits** with large scale data:\n",
        "\n",
        "- Storing 📋 instructions in memory takes **less space** than storing intermediate data results. If we are performing many operations on a dataset and are 🔙 returning the result data each step, we'll blow our storage faster although we don't need the intermediate results.\n",
        "\n",
        "- By having the list of operations to be performed, Spark can 💡 optimize the work between executors more efficiently."
      ],
      "metadata": {
        "id": "Kl9DrhkzegtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚡ Caching <a name=\"3.1\"></a>"
      ],
      "metadata": {
        "id": "x5KYvoH1A0vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs are by default ↩️ recomputed each time we run an 🎬 action on them. If we want to ♻️ reuse an RDD in multiple actions, we can **persist it in memory** (partitioned across the machines of the cluster). If we'll not reuse the RDD, there is 🙅 no reason to persist it since we would waste storage space when Spark could instead stream through the data once and just compute the result."
      ],
      "metadata": {
        "id": "qVsuLqRlA3_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ↕️ RDD lineage graph <a name=\"3.2\"></a>"
      ],
      "metadata": {
        "id": "cxwXT7Niy-pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we derive new RDDs from each other using transformations, Spark keeps 🗒 track of the set of 🖇 dependencies between different RDDs, called the **lineage graph**.\n",
        "\n",
        "<center><img src='https://i.ibb.co/7Wk1R6y/spark-lineage-graph-example.png'></center>\n",
        "\n",
        "An RDD lineage graph is hence a **graph of** what **transformations** need to be executed after an 🎬 action has been called. It starts with the 🔝 earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute."
      ],
      "metadata": {
        "id": "TYECHcOSzDmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏎🚌 Narrow and wide transformations <a name=\"3.3\"></a>"
      ],
      "metadata": {
        "id": "W7q4LlSpV3S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2️⃣ types of transformations:\n",
        "\n",
        "- 🏎 **Narrow transformations** are transformations where all the elements that are required to compute the records in a single partition live in the single partition of parent RDD. For example, `map` and `filter` are narrow transformations.\n",
        "\n",
        "- 🚌 **Wide transformations** are transformations where all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. For example, `groupbyKey` and `join` are wide transformations.\n",
        "\n",
        "\n",
        "<center><img src='https://i.ibb.co/qFQvqPV/narrow-and-wide-transformations-spark.png'></center>\n",
        "\n",
        "**Narrow** transformations are ✈ **faster** than wide transformations cause they **do not require any data 🔀 shuffling** over the cluster network or no data movement. It is always good to keep in mind transformations that might require data shuffling (and hence slow 🐢 down the process), and ⬇ reduce the usage of wide transformations if it's possible."
      ],
      "metadata": {
        "id": "Q0SyGQ_uWEsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Jobs, stages and tasks <a name=\"3.4\"></a>"
      ],
      "metadata": {
        "id": "EQ53x2lk4Gmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we invoke an 🎬 action on an RDD, a **job** is created. A job is divided into single or multiple stages, and stages are further ✂️ divided into individual tasks.\n",
        "- A **job is divided into stages** based on the 🔀 shuffle boundary. That is when Spark encounters a transformation that requires a shuffle (🚌 wide transformation) it creates a new stage.\n",
        "\n",
        "- Each stage is divided into tasks based on the 🔢 number of partitions in the RDD. \n",
        "\n",
        "- The **tasks** within a stage will **run in parallel. Every task** in the stage executes the same 📋 set of instructions **over a 1 single partition**. So tasks are the smallest units of work in Spark.\n",
        "\n",
        "<center><img src='https://i.ibb.co/Bzbwhk7/spark-job.png'></center>\n",
        "\n",
        "\n",
        "The figure below ilustrates an example that shows how Spark computes job stages. Boxes with solid outlines are RDDs. Partitions are shaded rectangles, in black if they are cached.\n",
        "\n",
        "<center><img src='https://i.ibb.co/c39WQMk/stages-spark.png'></center>\n",
        "\n",
        "To run an action on RDD *G*, the scheduler builds stages at 🚌 wide dependencies and **pipelines** 🏎 narrow transformations inside each stage. In this case, stage 1 does not need to run since B is cached, so we run 2 and then 3."
      ],
      "metadata": {
        "id": "5wi_W_z54J9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ↔️ DAG (Directed Acyclic Graph) <a name=\"3.5\"></a>"
      ],
      "metadata": {
        "id": "p-s467J5M7lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **DAG** (Directed Acyclic Graph) is a directed graph with no 🔄 directed cycles. In Spark, the ✳️ **vertices represent the RDDs**, and the ➖ **edges** represent the **operation** to be applied on RDD.\n",
        "\n",
        "During execution, Spark **transforms a ↕️ logical execution plan ([RDD lineage](#3.1)) to a physical execution plan (↔️ DAG of stages)** performing several ♻️ optimizations (such as \"pipelining\" transformations).\n",
        "\n",
        "\n",
        "<center><img src='https://i.ibb.co/0s0vy8M/dagscheduler-rdd-lineage-stage-dag.png'></center>"
      ],
      "metadata": {
        "id": "4-t1DFIvM9Ro"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvkMRedZ9B0G"
      },
      "source": [
        "# 🏗 Spark architecture <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj9uUhsc_atT"
      },
      "source": [
        "In distributed mode, Spark follows a **master-slave architecture** with a cluster manager. A cluster has a master node and several 👷 worker nodes. In the master node is where the process called 🚗 ***driver*** runs. The driver 🗣️ communicates with distributed workers which run processes called ⚙️ ***executors***.\n",
        "\n",
        "<center><img src='https://i.ibb.co/dkN1Tj6/spark-archi.png'></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚗 The driver <a name=\"4.1\"></a>"
      ],
      "metadata": {
        "id": "vJfswOll97yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 🚗 **driver program** is the process that runs the 👨‍💻 code of our application that creates a Spark Context, RDDs, and perform transformations and 🎬 actions.\n",
        "\n",
        "> The ***Spark Context*** is like a 🚪 **gateway to the Spark functionalities**. It's similar to a database connection. Any command we execute in our database goes through the database connection. Likewise, anything we do on Spark goes through Spark context.\n",
        "\n",
        "The driver perform two duties:\n",
        "- **Convert** the user **program ➡️ into tasks**. When the driver runs and find an action on an RDD, it will 🔜 trigger a job to be run. \n",
        "\n",
        "  The driver then **converts the lineage graph into a set of stages** performing several optimizations (such as \"pipelining\" transformations). The tasks of the stages are blunded up and prepared to be ➡️ sent to the cluster.\n",
        "\n",
        "- 🕒 **Schedule tasks on 👷 executors**. The driver, which has a complete 👀 view of the application's executors, coordinates the scheduling of individual tasks on executors. The drive will look at the current executors and try to schedule each task in an appropiate location, based on data placement.\n",
        "\n",
        "Moreover, the driver exposes information about the running Spark application through a **web interface**."
      ],
      "metadata": {
        "id": "sXoU_tk_k-85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 👷 Executors <a name=\"4.2\"></a>"
      ],
      "metadata": {
        "id": "6fBo9xyK9_pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The executors are worker processes responsible for 🔛 **running the individual tasks** in a given Spark job. Executors have two roles:\n",
        "\n",
        "- They run the tasks and ↩️ return results to the 🚗 driver.\n",
        "\n",
        "- They provide **in-memory storage** for RDDs that are cached by 👨‍💻 user programs. Since RDDs are cached inside the executors, tasks can run alongside the cached data.\n",
        "\n",
        "A driver and its executors are 🔗 together termed a ***Spark application***."
      ],
      "metadata": {
        "id": "f3-ou_B0lGGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🕹 Cluster manager <a name=\"4.3\"></a>"
      ],
      "metadata": {
        "id": "zWzvb9KG-Bsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A spark application is launched using an external service called cluster manager. The cluster manager launchs the 👷 executor processes. Spark can run 🔝 on top of different cluster managers, such as YARN, Mesos, and Standalone cluster manager."
      ],
      "metadata": {
        "id": "hElavflllHHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔛 Launching a program <a name=\"4.4\"></a>"
      ],
      "metadata": {
        "id": "5i4YPBpGKlHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark provides the script `spark-submit` to ⬆️ **submit a program**. Through various options, `spark-submit` can 🔗 connect to different cluster managers and 🕹 control how many resources the application gets. For some cluster managers, `spark-submit` can run the 🚗 driver within the cluster (e.g., on a YARN worker node), while for others, it can run it only on our 💻 local machine."
      ],
      "metadata": {
        "id": "hYWEivUKKpF6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Timw1k9B5Z"
      },
      "source": [
        "## 🔃 Workflow <a name=\"4.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's 🚶 walk through the steps that occur when we 🔛 run a Spark application on a cluster:\n",
        "\n",
        "1. The 👨‍💻 user submits an application using the `spark-submit` command.\n",
        "2. `spark-submit` launches the 🚗 driver program and invokes the `main()` method specified by the user.\n",
        "3. The 🚗 driver program 🔊 contacts the 🕹 cluster manager to ask for resources to launch executors.\n",
        "4. The cluster manager launches 👷 executors on behalf of the driver program.\n",
        "5. The 🚗 driver process runs through the user application. Based on the RDD 🎬 actions and ⚙️ transformations in the program, the driver ➡️ sends work to executors in the form of tasks.\n",
        "6. Tasks are run on 👷 executor processes to compute and save results or return them to the driver.\n",
        "7. If the driver’s `main()` method exits or it calls `SparkContext.stop()`, it will 🏁 terminate the executors and release resources from the cluster manager."
      ],
      "metadata": {
        "id": "WUjH2InvJxOV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH3crTci9B7v"
      },
      "source": [
        "# 📕 References <a name=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3qKBAf9DHi"
      },
      "source": [
        "- [Spark documentation](https://spark.apache.org/docs/latest/cluster-overview.html)\n",
        "\n",
        "- [Book \"Learning Spark: Lightning-Fast Big Data Analysis\"](https://www.oreilly.com/library/view/learning-spark/9781449359034/)\n",
        "\n",
        "- [Online book \"apache-spark-internals\"](https://books.japila.pl/apache-spark-internals/)\n",
        "\n",
        "- [Databricks documentation](https://docs.databricks.com/getting-started/spark/index.html)\n",
        "\n",
        "- [Book \"Data Analysis with Python and PySpark\"](https://www.manning.com/books/data-analysis-with-python-and-pyspark)\n",
        "\n",
        "- [A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)](https://youtu.be/dmL0N3qfSc8)\n",
        "\n",
        "- [Advanced Apache Spark Training - Sameer Farooqui (Databricks)](https://youtu.be/7ooZ4S7Ay6Y)"
      ]
    }
  ]
}