{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark RDD example: Jobs, stages and tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RxS50xG605m-",
        "sq8U3BtmhtRx",
        "esTZaJfDuYK4",
        "6cets4MGf-bA"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPPH70dmCu2aEw/bZSj198P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Big-Data/blob/main/PySpark_RDD_example_Jobs%2C_stages_and_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Example: Jobs, stages, and tasks"
      ],
      "metadata": {
        "id": "RxS50xG605m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we‚Äôve discussed in the notebook [Introduction to Spark](https://nbviewer.org/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb):\n",
        "\n",
        "- RDDs support 2Ô∏è‚É£ **two types of operations**: [üé¨‚öôÔ∏è actions and transformations](https://nbviewer.org/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb#3).\n",
        "\n",
        "- The components of ‚öôÔ∏è execution of a Spark application are: [**Jobs, stages, and tasks**](https://nbviewer.org/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb#%E2%9A%99%EF%B8%8F-Jobs,-stages-and-tasks-)\n",
        "\n",
        "We also discussed that, during executing, Spark **translates a ‚ÜïÔ∏è logical representation** (RDD Lineage) **into a physical execution plan** by performing several ‚ôªÔ∏è optimizations and merging multiple operations into tasks (‚ÜîÔ∏è[DAG](https://nbviewer.org/github/victorviro/Big-Data/blob/main/Introduction_to_Spark.ipynb#%E2%86%94%EF%B8%8F-DAG-(Directed-Acyclic-Graph)-)). Understanding how user code compiles ‚¨á down into the üéö level of physical execution is an advanced concept, but it helps us in tuning and üêû debugging applications.\n",
        "\n",
        "To demonstrate Spark‚Äôs phases of execution, we‚Äôll walk through an **example** application of a **simple log analysis**. \n",
        "\n",
        "The table of contents of this notebook is as follow:\n",
        "\n",
        "1. [üì• Running Pyspark in¬†Colab](#1)\n",
        "2. [üèãÔ∏è Exercise](#2)\n",
        "    1. [‚ö° Caching](#2.1)\n",
        "    2. [üìã Spark execution summary](#2.2)\n",
        "5. [üìï References](#3)"
      ],
      "metadata": {
        "id": "D6tNJjyx5Fd8"
      }
    },
    {
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "cell_type": "markdown",
      "source": [
        "# üì• Running Pyspark in¬†Colab <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run spark in Colab, we need to first install all the dependencies, i.e. Apache Spark with hadoop, Java 8 and Findspark to locate the spark in the system. Follow the steps to install the dependencies:"
      ],
      "metadata": {
        "id": "U7SB2ODXnSiw"
      }
    },
    {
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "!tar xvf \"spark-3.0.3-bin-hadoop3.2.tgz\"\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we installed Spark and Java, we set the environment path which enables us to run Pyspark."
      ]
    },
    {
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's run a local spark session to test our installation:"
      ]
    },
    {
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init('/content/spark-3.0.3-bin-hadoop3.2')\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9nK1zrJqNUB",
        "outputId": "3ad380bb-f0d2-49a8-e871-f7bdd9119d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f2e6b83c550>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Exercise <a name=\"2\"></a>"
      ],
      "metadata": {
        "id": "esTZaJfDuYK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **input data**, we‚Äôll use a text file that consists of log messages of varying üéö degrees of severity. We want to compute **how many log messages appear at each üéö level of severity**.\n",
        "\n",
        "\n",
        "Let's üì• download and print its first lines."
      ],
      "metadata": {
        "id": "3KPsO3Ny8kso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/victorviro/Big-Data/main/files/logs.log\n",
        "!head -5 logs.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PsgnPv2kfnh",
        "outputId": "908b66b0-589a-494e-c85c-acfdb353e167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015-10-18 18:01:47,978 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445144423722_0020_000001\n",
            "2015-10-18 18:01:48,963 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens:\n",
            "2015-10-18 18:01:48,963 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 20 cluster_timestamp: 1445144423722 } attemptId: 1 } keyId: -127633188)\n",
            "2015-10-18 18:01:49,228 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter.\n",
            "2015-10-18 18:01:50,353 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = sc.textFile('logs.log')\n",
        "for line in input.take(2):\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvBMxH1SkwMW",
        "outputId": "26345d47-8397-4817-fca3-a988e9f75bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015-10-18 18:01:47,978 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445144423722_0020_000001\n",
            "2015-10-18 18:01:48,963 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, each element of the `input` RDD is a string representing a line of the logs file, that is a log message.\n",
        "\n",
        "Now let's ‚úÇÔ∏è split each log message into words."
      ],
      "metadata": {
        "id": "tXYdjNCbnb85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = input.filter(lambda x: len(x) > 0).map(lambda x: x.split(\" \"))"
      ],
      "metadata": {
        "id": "Vy-SoCRZlm1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `map()` operation does not mutate the existing `input` RDD. Instead, it returns a üëâ pointer to an entirely new RDD. `input` RDD can still be reused later in the program.\n",
        "\n",
        "Each element of the `tokenized` RDD is a list of strings representing a log message of the file."
      ],
      "metadata": {
        "id": "CyOG6gQ3nI7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for line in tokenized.take(2):\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPDYpi7j1ykd",
        "outputId": "401d307e-d185-4b76-8821-7577e46b2ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2015-10-18', '18:01:47,978', 'INFO', '[main]', 'org.apache.hadoop.mapreduce.v2.app.MRAppMaster:', 'Created', 'MRAppMaster', 'for', 'application', 'appattempt_1445144423722_0020_000001']\n",
            "['2015-10-18', '18:01:48,963', 'INFO', '[main]', 'org.apache.hadoop.mapreduce.v2.app.MRAppMaster:', 'Executing', 'with', 'tokens:']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we üî¢ count the number of messages for each üéö level of severity."
      ],
      "metadata": {
        "id": "-SAuCDwu2cel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "counts = tokenized.map(lambda x: (x[2], 1)).reduceByKey(add)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXICwK5Tog71",
        "outputId": "7e4d4c32-c04a-45da-fee5-dae6f2699f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INFO', 1040), ('WARN', 808), ('FATAL', 2), ('ERROR', 150)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To **display the lineage** of an RDD, Spark provides the `toDebugString()` method. Let's display the lineage of the `input` RDD."
      ],
      "metadata": {
        "id": "HicPLmBf3QTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input.toDebugString().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_h-M9Th3Zlo",
        "outputId": "3cbee470-dd59-4456-8cbf-c6eced401cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) logs.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
            " |  logs.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created the `input` RDD by calling `sc.textFile()`. The lineage gives us some clues as to what `sc.textFile()` does since it reveals which RDDs were created in the `textFile()` function. We can see that it creates a `HadoopRDD` and then performs a map on it to create the returned RDD.\n",
        "\n",
        "Now let's display the lineage of the `counts` RDD."
      ],
      "metadata": {
        "id": "S6-clsum3Zro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(counts.toDebugString().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PvyeEGkpnPY",
        "outputId": "e9c66784-8ca9-4982-fc9f-69ee1a6c2798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2) PythonRDD[8] at collect at <ipython-input-10-e54d4d27e4f7>:3 []\n",
            " |  MapPartitionsRDD[7] at mapPartitions at PythonRDD.scala:133 []\n",
            " |  ShuffledRDD[6] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
            " +-(2) PairwiseRDD[5] at reduceByKey at <ipython-input-10-e54d4d27e4f7>:2 []\n",
            "    |  PythonRDD[4] at reduceByKey at <ipython-input-10-e54d4d27e4f7>:2 []\n",
            "    |  logs.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
            "    |  logs.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ‚ÜïÔ∏è lineage of the `counts` RDD is more complicated. That RDD has several üëÜ ancestors since there are other operations that were performed on üîù top of the `input` RDD, such as additional maps, filtering, and reduction. The lineage of `counts` is displayed graphically in the following figure.\n",
        "\n",
        "<center><img src='https://i.ibb.co/hg4jHx1/rdd-lineage2.png'></center>\n",
        "\n",
        "Before we perform an action, these RDDs simply store metadata. To üîõ trigger computation, let‚Äôs call an üé¨ action on the `counts` RDD by `collect()`ing it to the üöó driver."
      ],
      "metadata": {
        "id": "mg1CbgJA4409"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBIzWa5Klvq9",
        "outputId": "5a6293a0-3246-44b4-91a1-b85ca72c893d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INFO', 1040), ('WARN', 808), ('FATAL', 2), ('ERROR', 150)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the number of log messages that appear at each üéö level of severity.\n",
        "\n",
        "Internally, Spark‚Äôs scheduler creates a **‚ÜîÔ∏è physical execution plan** to compute the RDDs needed for performing the üé¨ action. When we call `collect()` on the RDD, every partition of the RDD must be materialized and then transferred to the üöó driver program. Spark‚Äôs scheduler starts at the final RDD being computed (in this case, `counts`) and works ‚§¥Ô∏è backward to find what it must compute.\n",
        "\n",
        "In the simplest case, the scheduler outputs a computation stage for each RDD in this graph where the stage has tasks for each partition in that RDD. Those stages are then executed in ‚¨ÖÔ∏è reverse order to compute the final required RDD. In more complex cases, the physical set of stages will not be an exact üñá 1:1 correspondence to the RDD graph. This can occur when the scheduler performs üîó **pipelining (collapsing of multiple RDDs into a single-stage)**. Pipelining occurs when RDDs can be computed from their parents without data movement (no data üîÄ shuffling over the network is required).\n",
        "\n",
        "The ‚ÜïÔ∏è lineage output shown previously (by using the `toDebugString()` method) uses indentation levels to show where RDDs are going to be üîó pipelined together into physical stages. RDDs that exist at the same level of indentation as their parents will be pipelined during physical execution. For instance, when we are computing `counts`, even though there are a large number of parent RDDs, there are only two levels of indentation shown. This indicates that the physical execution will require only **two stages**. The pipelining, in this case, is because there are several filter and map operations in sequence. The right half of the following figure shows the two stages of execution that are required to compute the `counts` RDD.\n",
        "\n",
        "<center><img src='https://i.ibb.co/M1gcY95/spark-lineage-to-dag.png'></center>"
      ],
      "metadata": {
        "id": "pUk_pNYulh1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we visit the application‚Äôs web UI, we will see that two stages occur to fulfill the `collect()` üé¨ action.\n",
        "\n",
        "The **set of stages** produced for a particular action is termed a **job**. In each case **when we invoke üé¨ actions** such as `count()`, we are creating a job composed of one or more stages.\n",
        "\n",
        "Once the stage graph is defined, tasks are created and dispatched to an internal scheduler, which varies depending on the üöÄ deployment mode being used. Stages in the physical plan can üñá depend on each other, based on the RDD ‚ÜïÔ∏è lineage, so they will be executed in a specific order. For instance, a stage that outputs üîÄ shuffle data must occur ‚¨ÖÔ∏è before one that relies on that data being present.\n",
        "\n",
        "A physical **stage will launch tasks that each do the same thing but on specific partitions** of data. Each task internally performs the same steps:\n",
        "\n",
        "1. ‚¨ÖÔ∏è **Fetching its input**, either from data storage (if the RDD is an input RDD), an existing RDD (if the stage is based on already cached data), or shuffle outputs.\n",
        "\n",
        "2. **Performing the ‚öôÔ∏è operation** necessary to compute RDD(s) that it represents. For instance, executing `filter()` or `map()` functions on the input data, or performing grouping or reduction.\n",
        "\n",
        "3. ‚úçÔ∏è **Writing output to** a shuffle, to external storage, or back to the üöó driver (if it is the final RDD of an üé¨ action such as `count()`)."
      ],
      "metadata": {
        "id": "FfqTrVLdDxrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Caching <a name=\"2.1\"></a>"
      ],
      "metadata": {
        "id": "JTtNa9JRDxuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to üîó pipelining, \n",
        "\n",
        "- Spark‚Äôs internal scheduler may truncate the lineage of the RDD graph if an existing RDD has already been persisted in memory or on disk. Spark can ‚Äúshort-circuit‚Äù in this case and just begin computing based on the persisted RDD. \n",
        "\n",
        "- This truncation can happen also when an RDD is already materialized as a side effect of an earlier üîÄ shuffle, even if it was not explicitly `persist()`ed. This is an under-the-hood ‚ôªÔ∏è optimization that takes advantage of the fact that Spark shuffle outputs are written to disk, and exploits the fact that many times portions of the RDD graph are recomputed.\n",
        "\n",
        "To see the effects of caching on physical execution, let‚Äôs ‚ö° cache the `counts` RDD and see how that truncates the execution graph for future üé¨ actions. If we revisit the UI, we'll see that caching ‚¨á reduces the number of stages required when executing future computations. Calling `collect()` a few more times will reveal only one stage executing to perform the action."
      ],
      "metadata": {
        "id": "fr-dI1848Nsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the RDD\n",
        "counts.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f-fzEDblmYR",
        "outputId": "15542773-46a7-4152-b4f5-0d39a35575d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[8] at collect at <ipython-input-10-e54d4d27e4f7>:3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The first subsequent execution after caching will again require 2 stages\n",
        "counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH8gDwGB9_fC",
        "outputId": "1635e1e9-1cb9-4d12-a972-107a3512957a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('INFO', 1040), ('WARN', 808), ('FATAL', 2), ('ERROR', 150)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This execution will only require a single stage\n",
        "counts.collect()"
      ],
      "metadata": {
        "id": "L3l5sAof9_kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã Spark execution summary <a name=\"2.2\"></a>"
      ],
      "metadata": {
        "id": "_PajSZXKFVkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize, the following phases occur during Spark execution:\n",
        "- üë®‚Äçüíª **User** code **defines** the Spark **application**\n",
        "- Operations on RDDs create new RDDs that refer back to their parents, thereby creating the ‚ÜîÔ∏è **lineage graph**.\n",
        "- üé¨ Actions force **translation** of the logical representation **to a physical execution plan (DAG)**.\n",
        "- **When we call an üé¨ action** on an RDD it must be computed. This requires computing its parent RDDs as well. Spark‚Äôs scheduler submits a **job** to compute all needed RDDs. That job will **have one or more stages**, which are parallel waves of computation **composed of tasks**. Each stage will correspond to one or more RDDs in the DAG. A single-stage can correspond to multiple RDDs due to üîó pipelining.\n",
        "- **Tasks are ‚åöÔ∏è scheduled** and executed on a cluster.\n",
        "- Stages are processed in order, with individual tasks launching to compute segments of the RDD. Once the final stage is finished in a job, the action is complete üîö.\n",
        "\n",
        "In a given Spark application, this entire sequence of steps may occur many times in a continuous fashion as new RDDs are created."
      ],
      "metadata": {
        "id": "JgxGx3SQFUuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìï References <a name=\"3\"></a>"
      ],
      "metadata": {
        "id": "6cets4MGf-bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Spark documentation](https://spark.apache.org/docs/latest/cluster-overview.html)\n",
        "\n",
        "- [Book \"Learning Spark: Lightning-Fast Big Data Analysis\"](https://www.oreilly.com/library/view/learning-spark/9781449359034/)"
      ],
      "metadata": {
        "id": "T9on1CHdMoq3"
      }
    }
  ]
}